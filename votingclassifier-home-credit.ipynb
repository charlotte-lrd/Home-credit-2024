{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5638706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:18:35.302464Z",
     "iopub.status.busy": "2024-05-26T13:18:35.302196Z",
     "iopub.status.idle": "2024-05-26T13:18:35.320427Z",
     "shell.execute_reply": "2024-05-26T13:18:35.319483Z"
    },
    "papermill": {
     "duration": 0.030079,
     "end_time": "2024-05-26T13:18:35.322274",
     "exception": false,
     "start_time": "2024-05-26T13:18:35.292195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "class Pipeline:\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df):\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "    \n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max\n",
    "    \n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max\n",
    "    \n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max \n",
    "    \n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n",
    "        return  expr_max\n",
    "    \n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n",
    "\n",
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2    \n",
    "    return df\n",
    "\n",
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_train = feature_eng(**data_store)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "nums=df_train.select_dtypes(exclude='category').columns\n",
    "from itertools import combinations, permutations\n",
    "nans_df = df_train[nums].isna()\n",
    "nans_groups={}\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "del nans_df; x=gc.collect()\n",
    "\n",
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df_train[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "        use.append(vx)\n",
    "    return use\n",
    "\n",
    "def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "    correlation_matrix = matrix.corr()\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "    else:\n",
    "        uses=uses+v\n",
    "df_train=df_train[uses]\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test = feature_eng(**data_store)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n",
    "df_test, cat_cols = to_pandas(df_test)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "gc.collect()\n",
    "\n",
    "df_train['target']=0\n",
    "df_test['target']=1\n",
    "\n",
    "df_train=pd.concat([df_train,df_test])\n",
    "df_train=reduce_mem_usage(df_train)\n",
    "\n",
    "y = df_train[\"target\"]\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "\n",
    "joblib.dump((df_train,y,df_test),'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb8ff88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:18:35.340431Z",
     "iopub.status.busy": "2024-05-26T13:18:35.340159Z",
     "iopub.status.idle": "2024-05-26T13:21:14.791179Z",
     "shell.execute_reply": "2024-05-26T13:21:14.789942Z"
    },
    "papermill": {
     "duration": 159.463476,
     "end_time": "2024-05-26T13:21:14.793799",
     "exception": false,
     "start_time": "2024-05-26T13:18:35.330323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9233bfe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.811944Z",
     "iopub.status.busy": "2024-05-26T13:21:14.811621Z",
     "iopub.status.idle": "2024-05-26T13:21:14.818011Z",
     "shell.execute_reply": "2024-05-26T13:21:14.817234Z"
    },
    "papermill": {
     "duration": 0.01767,
     "end_time": "2024-05-26T13:21:14.819935",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.802265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile baseline.py\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e98c624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.837160Z",
     "iopub.status.busy": "2024-05-26T13:21:14.836579Z",
     "iopub.status.idle": "2024-05-26T13:21:14.841973Z",
     "shell.execute_reply": "2024-05-26T13:21:14.841088Z"
    },
    "papermill": {
     "duration": 0.016145,
     "end_time": "2024-05-26T13:21:14.844155",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.828010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "# Set a seed for various non-deterministic processes for reproducibility\n",
    "import random\n",
    "def seed_it_all(seed=7):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# set the seed for this run\n",
    "seed_it_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23780e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.862176Z",
     "iopub.status.busy": "2024-05-26T13:21:14.861685Z",
     "iopub.status.idle": "2024-05-26T13:21:14.871468Z",
     "shell.execute_reply": "2024-05-26T13:21:14.870539Z"
    },
    "papermill": {
     "duration": 0.021257,
     "end_time": "2024-05-26T13:21:14.873523",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.852266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.7:\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        \n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return expr_max +expr_last+expr_mean\n",
    "    \n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last+expr_mean\n",
    "    \n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last#+expr_count\n",
    "    \n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last\n",
    "    \n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last\n",
    "    \n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n",
    "\n",
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074706e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.892287Z",
     "iopub.status.busy": "2024-05-26T13:21:14.892017Z",
     "iopub.status.idle": "2024-05-26T13:21:14.897368Z",
     "shell.execute_reply": "2024-05-26T13:21:14.896446Z"
    },
    "papermill": {
     "duration": 0.017604,
     "end_time": "2024-05-26T13:21:14.899361",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.881757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c7199e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.918209Z",
     "iopub.status.busy": "2024-05-26T13:21:14.917576Z",
     "iopub.status.idle": "2024-05-26T13:21:14.923410Z",
     "shell.execute_reply": "2024-05-26T13:21:14.922580Z"
    },
    "papermill": {
     "duration": 0.017189,
     "end_time": "2024-05-26T13:21:14.925295",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.908106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc26512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.945184Z",
     "iopub.status.busy": "2024-05-26T13:21:14.944887Z",
     "iopub.status.idle": "2024-05-26T13:21:14.951421Z",
     "shell.execute_reply": "2024-05-26T13:21:14.950616Z"
    },
    "papermill": {
     "duration": 0.018534,
     "end_time": "2024-05-26T13:21:14.953421",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.934887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "df_train = feature_eng(**data_store)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "nums=df_train.select_dtypes(exclude='category').columns\n",
    "from itertools import combinations, permutations\n",
    "#df_train=df_train[nums]\n",
    "nans_df = df_train[nums].isna()\n",
    "nans_groups={}\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "del nans_df; x=gc.collect()\n",
    "\n",
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df_train[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "            #print(str(gg)+'-'+str(n),', ',end='')\n",
    "        use.append(vx)\n",
    "        #print()\n",
    "    print('Use these',use)\n",
    "    return use\n",
    "\n",
    "def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "    # 计算列之间的相关性\n",
    "    correlation_matrix = matrix.corr()\n",
    "\n",
    "    # 分组列\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            #cross_features=list(combinations(Vs, 2))\n",
    "            #make_corr(Vs)\n",
    "            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "            #make_corr(use)\n",
    "    else:\n",
    "        uses=uses+v\n",
    "    print('####### NAN count =',k)\n",
    "print(uses)\n",
    "print(len(uses))\n",
    "uses=uses+list(df_train.select_dtypes(include='category').columns)\n",
    "print(len(uses))\n",
    "df_train=df_train[uses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d09347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.971431Z",
     "iopub.status.busy": "2024-05-26T13:21:14.970963Z",
     "iopub.status.idle": "2024-05-26T13:21:14.975802Z",
     "shell.execute_reply": "2024-05-26T13:21:14.974989Z"
    },
    "papermill": {
     "duration": 0.015829,
     "end_time": "2024-05-26T13:21:14.977598",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.961769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\n",
    "device='gpu'\n",
    "n_est=6000\n",
    "DRY_RUN = True if sample.shape[0] == 10 else False   \n",
    "if DRY_RUN:\n",
    "    device='cpu'\n",
    "    df_train = df_train.iloc[:50000]\n",
    "    n_est=600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3587dfbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:14.996200Z",
     "iopub.status.busy": "2024-05-26T13:21:14.995936Z",
     "iopub.status.idle": "2024-05-26T13:21:15.001751Z",
     "shell.execute_reply": "2024-05-26T13:21:15.000841Z"
    },
    "papermill": {
     "duration": 0.01701,
     "end_time": "2024-05-26T13:21:15.003522",
     "exception": false,
     "start_time": "2024-05-26T13:21:14.986512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f73dd29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.021504Z",
     "iopub.status.busy": "2024-05-26T13:21:15.021258Z",
     "iopub.status.idle": "2024-05-26T13:21:15.026506Z",
     "shell.execute_reply": "2024-05-26T13:21:15.025618Z"
    },
    "papermill": {
     "duration": 0.01637,
     "end_time": "2024-05-26T13:21:15.028360",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.011990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "df_test = feature_eng(**data_store)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a42450",
   "metadata": {
    "papermill": {
     "duration": 0.008229,
     "end_time": "2024-05-26T13:21:15.045098",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.036869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b791ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.063227Z",
     "iopub.status.busy": "2024-05-26T13:21:15.062808Z",
     "iopub.status.idle": "2024-05-26T13:21:15.067820Z",
     "shell.execute_reply": "2024-05-26T13:21:15.066940Z"
    },
    "papermill": {
     "duration": 0.016128,
     "end_time": "2024-05-26T13:21:15.069775",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.053647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3093b7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.088102Z",
     "iopub.status.busy": "2024-05-26T13:21:15.087857Z",
     "iopub.status.idle": "2024-05-26T13:21:15.092675Z",
     "shell.execute_reply": "2024-05-26T13:21:15.091828Z"
    },
    "papermill": {
     "duration": 0.016242,
     "end_time": "2024-05-26T13:21:15.094602",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.078360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "df_train[cat_cols] = df_train[cat_cols].astype(str)\n",
    "df_test[cat_cols] = df_test[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54b54c18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.112916Z",
     "iopub.status.busy": "2024-05-26T13:21:15.112645Z",
     "iopub.status.idle": "2024-05-26T13:21:15.117641Z",
     "shell.execute_reply": "2024-05-26T13:21:15.116852Z"
    },
    "papermill": {
     "duration": 0.01634,
     "end_time": "2024-05-26T13:21:15.119671",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.103331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "params_lgb = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 10,  \n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 2500,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": SEED,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':64,\n",
    "    \"device\": device, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e67ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.138267Z",
     "iopub.status.busy": "2024-05-26T13:21:15.138021Z",
     "iopub.status.idle": "2024-05-26T13:21:15.142924Z",
     "shell.execute_reply": "2024-05-26T13:21:15.142107Z"
    },
    "papermill": {
     "duration": 0.016337,
     "end_time": "2024-05-26T13:21:15.144818",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.128481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "params_lgb2 = {\n",
    "    \"boosting_type\": \"goss\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 10,  \n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 2500,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": SEED,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':64,\n",
    "    \"device\": device, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2a7f4ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.163786Z",
     "iopub.status.busy": "2024-05-26T13:21:15.163500Z",
     "iopub.status.idle": "2024-05-26T13:21:15.169230Z",
     "shell.execute_reply": "2024-05-26T13:21:15.168429Z"
    },
    "papermill": {
     "duration": 0.017577,
     "end_time": "2024-05-26T13:21:15.171248",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.153671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "fitted_models_cb = []\n",
    "fitted_models_lgb = []\n",
    "fitted_models_lgb2 = []\n",
    "fitted_models_eclf = []\n",
    "cv_scores_cb = []\n",
    "cv_scores_lgb = []\n",
    "cv_scores_lgb2 = []\n",
    "cv_scores_eclf = []\n",
    "\n",
    "for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n",
    "    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n",
    "    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n",
    "    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n",
    "    clf_cb = CatBoostClassifier(\n",
    "        eval_metric='AUC',\n",
    "        task_type='GPU',\n",
    "        learning_rate=0.05,\n",
    "        iterations=n_est)\n",
    "    random_seed=SEED\n",
    "    clf_cb.fit(train_pool, eval_set=val_pool,verbose=300)\n",
    "    fitted_models_cb.append(clf_cb)\n",
    "    y_pred_valid = clf_cb.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_cb.append(auc_score)\n",
    "    \n",
    "    \n",
    "    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n",
    "    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n",
    "    \n",
    "    clf_lgb = LGBMClassifier(**params_lgb)\n",
    "    clf_lgb.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_valid, y_valid)],\n",
    "        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n",
    "    \n",
    "    fitted_models_lgb.append(clf_lgb)\n",
    "    y_pred_valid = clf_lgb.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_lgb.append(auc_score)\n",
    "    \n",
    "    clf_lgb2 = LGBMClassifier(**params_lgb2)\n",
    "    clf_lgb2.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_valid, y_valid)],\n",
    "        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n",
    "    \n",
    "    fitted_models_lgb2.append(clf_lgb2)\n",
    "    y_pred_valid = clf_lgb2.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_lgb2.append(auc_score)\n",
    " \n",
    "    eclf = VotingClassifier(\n",
    "     estimators=[('lgb', clf_lgb), ('lgb2', clf_lgb2)],\n",
    "     voting='soft', weights=[1, 1])   \n",
    "    eclf = eclf.fit(X_train, y_train)\n",
    "    fitted_models_eclf.append(eclf)\n",
    "    y_pred_valid = eclf.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_eclf.append(auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c974a9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.190500Z",
     "iopub.status.busy": "2024-05-26T13:21:15.190027Z",
     "iopub.status.idle": "2024-05-26T13:21:15.195042Z",
     "shell.execute_reply": "2024-05-26T13:21:15.194259Z"
    },
    "papermill": {
     "duration": 0.016579,
     "end_time": "2024-05-26T13:21:15.196952",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.180373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "print(\"CatBoost\")   \n",
    "print(\"CV AUC scores: \", cv_scores_cb)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores_cb))\n",
    "print(\"LightGBM\")\n",
    "print(\"CV AUC scores: \", cv_scores_lgb)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores_lgb))\n",
    "print(\"LightGBM_goss\")\n",
    "print(\"CV AUC scores: \", cv_scores_lgb2)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores_lgb2))\n",
    "print(\"Ensemble of LGBM and LGBM_goss\")\n",
    "print(\"CV AUC scores: \", cv_scores_eclf)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores_eclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1266243f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.216204Z",
     "iopub.status.busy": "2024-05-26T13:21:15.215676Z",
     "iopub.status.idle": "2024-05-26T13:21:15.220626Z",
     "shell.execute_reply": "2024-05-26T13:21:15.219863Z"
    },
    "papermill": {
     "duration": 0.016596,
     "end_time": "2024-05-26T13:21:15.222635",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.206039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n",
    "        \n",
    "        X[cat_cols] = X[cat_cols].astype(\"category\")\n",
    "        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[5:]]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "model = VotingModel(fitted_models_cb+fitted_models_eclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1d3f0",
   "metadata": {
    "papermill": {
     "duration": 0.00879,
     "end_time": "2024-05-26T13:21:15.240531",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.231741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7bb707e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.260321Z",
     "iopub.status.busy": "2024-05-26T13:21:15.259885Z",
     "iopub.status.idle": "2024-05-26T13:21:15.265083Z",
     "shell.execute_reply": "2024-05-26T13:21:15.264113Z"
    },
    "papermill": {
     "duration": 0.017237,
     "end_time": "2024-05-26T13:21:15.267018",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.249781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a baseline.py\n",
    "\n",
    "df_test = df_test.drop(columns=[\"WEEK_NUM\"])\n",
    "df_test = df_test.set_index(\"case_id\")\n",
    "y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n",
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm[\"score\"] = y_pred\n",
    "df_subm.to_csv(\"sub.csv\")\n",
    "df_subm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf1934d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:21:15.286180Z",
     "iopub.status.busy": "2024-05-26T13:21:15.285922Z",
     "iopub.status.idle": "2024-05-26T13:48:55.565544Z",
     "shell.execute_reply": "2024-05-26T13:48:55.564034Z"
    },
    "papermill": {
     "duration": 1660.293102,
     "end_time": "2024-05-26T13:48:55.569244",
     "exception": false,
     "start_time": "2024-05-26T13:21:15.276142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\t (1526659, 861)\r\n",
      "Memory usage of dataframe is 4322.75 MB\r\n",
      "Memory usage after optimization is: 1528.81 MB\r\n",
      "Decreased by 64.6%\r\n",
      "train data shape:\t (1526659, 472)\r\n",
      "Use these ['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9']\r\n",
      "####### NAN count = 0\r\n",
      "####### NAN count = 918788\r\n",
      "Use these ['dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'mean_refreshdate_3813885D']\r\n",
      "####### NAN count = 140968\r\n",
      "Use these ['pmtscount_423L', 'pmtssum_45A']\r\n",
      "####### NAN count = 954021\r\n",
      "####### NAN count = 806659\r\n",
      "####### NAN count = 866332\r\n",
      "####### NAN count = 418178\r\n",
      "Use these ['amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L']\r\n",
      "####### NAN count = 561124\r\n",
      "Use these ['annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A']\r\n",
      "####### NAN count = 4\r\n",
      "Use these ['mindbddpdlast24m_3658935P']\r\n",
      "####### NAN count = 613202\r\n",
      "####### NAN count = 948244\r\n",
      "Use these ['mindbdtollast24m_4525191P']\r\n",
      "####### NAN count = 972827\r\n",
      "####### NAN count = 467175\r\n",
      "Use these ['avginstallast24m_3658937A', 'maxinstallast24m_3658928A']\r\n",
      "####### NAN count = 624875\r\n",
      "####### NAN count = 757006\r\n",
      "####### NAN count = 841181\r\n",
      "####### NAN count = 1026987\r\n",
      "####### NAN count = 455190\r\n",
      "####### NAN count = 460822\r\n",
      "Use these ['commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P']\r\n",
      "####### NAN count = 343375\r\n",
      "####### NAN count = 833735\r\n",
      "####### NAN count = 887659\r\n",
      "Use these ['daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L']\r\n",
      "####### NAN count = 452594\r\n",
      "####### NAN count = 977119\r\n",
      "Use these ['eir_270L']\r\n",
      "####### NAN count = 190833\r\n",
      "####### NAN count = 859214\r\n",
      "####### NAN count = 482103\r\n",
      "####### NAN count = 453587\r\n",
      "Use these ['lastapplicationdate_877D', 'mean_creationdate_885D', 'max_num_group1', 'last_num_group1', 'max_num_group2_14', 'last_num_group2_14']\r\n",
      "####### NAN count = 305137\r\n",
      "Use these ['lastapprcredamount_781A', 'lastapprdate_640D']\r\n",
      "####### NAN count = 442041\r\n",
      "####### NAN count = 977975\r\n",
      "Use these ['lastrejectcredamount_222A', 'lastrejectdate_50D']\r\n",
      "####### NAN count = 769046\r\n",
      "####### NAN count = 511255\r\n",
      "Use these ['mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P']\r\n",
      "####### NAN count = 306019\r\n",
      "####### NAN count = 960953\r\n",
      "####### NAN count = 705504\r\n",
      "####### NAN count = 876276\r\n",
      "####### NAN count = 826000\r\n",
      "####### NAN count = 829402\r\n",
      "####### NAN count = 1032856\r\n",
      "####### NAN count = 766958\r\n",
      "Use these ['numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L']\r\n",
      "####### NAN count = 452593\r\n",
      "####### NAN count = 455081\r\n",
      "Use these ['numinstlsallpaid_934L']\r\n",
      "####### NAN count = 445669\r\n",
      "Use these ['numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L']\r\n",
      "####### NAN count = 456495\r\n",
      "Use these ['numinstpaid_4499208L']\r\n",
      "####### NAN count = 847191\r\n",
      "####### NAN count = 446983\r\n",
      "Use these ['numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A']\r\n",
      "####### NAN count = 840646\r\n",
      "####### NAN count = 669186\r\n",
      "####### NAN count = 455612\r\n",
      "Use these ['pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L']\r\n",
      "####### NAN count = 458738\r\n",
      "####### NAN count = 461362\r\n",
      "####### NAN count = 459827\r\n",
      "####### NAN count = 460079\r\n",
      "####### NAN count = 44954\r\n",
      "####### NAN count = 78526\r\n",
      "####### NAN count = 131888\r\n",
      "####### NAN count = 181122\r\n",
      "####### NAN count = 223240\r\n",
      "####### NAN count = 445320\r\n",
      "####### NAN count = 3\r\n",
      "Use these ['mean_actualdpd_943P']\r\n",
      "####### NAN count = 305154\r\n",
      "Use these ['max_annuity_853A', 'mean_annuity_853A']\r\n",
      "####### NAN count = 308739\r\n",
      "Use these ['max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A']\r\n",
      "####### NAN count = 307441\r\n",
      "Use these ['max_currdebt_94A', 'mean_currdebt_94A']\r\n",
      "####### NAN count = 419006\r\n",
      "Use these ['max_mainoccupationinc_437A', 'mean_mainoccupationinc_437A']\r\n",
      "####### NAN count = 306361\r\n",
      "Use these ['mean_maxdpdtolerance_577P']\r\n",
      "####### NAN count = 450969\r\n",
      "Use these ['max_outstandingdebt_522A', 'mean_outstandingdebt_522A']\r\n",
      "####### NAN count = 420383\r\n",
      "####### NAN count = 307551\r\n",
      "####### NAN count = 477657\r\n",
      "####### NAN count = 433335\r\n",
      "Use these ['last_credamount_590A', 'last_downpmt_134A']\r\n",
      "####### NAN count = 438219\r\n",
      "####### NAN count = 824731\r\n",
      "####### NAN count = 312491\r\n",
      "####### NAN count = 899665\r\n",
      "####### NAN count = 827764\r\n",
      "Use these ['max_approvaldate_319D', 'mean_approvaldate_319D']\r\n",
      "####### NAN count = 442999\r\n",
      "Use these ['max_dateactivated_425D', 'mean_dateactivated_425D']\r\n",
      "####### NAN count = 454678\r\n",
      "Use these ['max_dtlastpmt_581D', 'mean_dtlastpmt_581D']\r\n",
      "####### NAN count = 703840\r\n",
      "Use these ['max_dtlastpmtallstes_3545839D', 'mean_dtlastpmtallstes_3545839D']\r\n",
      "####### NAN count = 548987\r\n",
      "Use these ['max_employedfrom_700D']\r\n",
      "####### NAN count = 559169\r\n",
      "Use these ['max_firstnonzeroinstldate_307D', 'mean_firstnonzeroinstldate_307D']\r\n",
      "####### NAN count = 334873\r\n",
      "####### NAN count = 891021\r\n",
      "####### NAN count = 305203\r\n",
      "####### NAN count = 920818\r\n",
      "####### NAN count = 1016761\r\n",
      "####### NAN count = 1050001\r\n",
      "####### NAN count = 485683\r\n",
      "####### NAN count = 961606\r\n",
      "####### NAN count = 552766\r\n",
      "Use these ['max_pmtnum_8L']\r\n",
      "####### NAN count = 321446\r\n",
      "Use these ['last_pmtnum_8L']\r\n",
      "####### NAN count = 482174\r\n",
      "Use these ['max_pmtamount_36A', 'last_pmtamount_36A', 'max_processingdate_168D', 'last_processingdate_168D', 'max_num_group1_5']\r\n",
      "####### NAN count = 1044394\r\n",
      "Use these ['mean_credlmt_230A']\r\n",
      "####### NAN count = 1036944\r\n",
      "Use these ['mean_credlmt_935A']\r\n",
      "####### NAN count = 603001\r\n",
      "Use these ['mean_pmts_dpd_1073P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T']\r\n",
      "####### NAN count = 263166\r\n",
      "Use these ['max_pmts_dpd_303P', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P']\r\n",
      "####### NAN count = 514070\r\n",
      "Use these ['mean_instlamount_768A']\r\n",
      "####### NAN count = 606920\r\n",
      "Use these ['mean_monthlyinstlamount_332A']\r\n",
      "####### NAN count = 263233\r\n",
      "Use these ['max_monthlyinstlamount_674A', 'mean_monthlyinstlamount_674A']\r\n",
      "####### NAN count = 517511\r\n",
      "Use these ['mean_outstandingamount_354A']\r\n",
      "####### NAN count = 545885\r\n",
      "Use these ['mean_outstandingamount_362A']\r\n",
      "####### NAN count = 636453\r\n",
      "Use these ['mean_overdueamount_31A']\r\n",
      "####### NAN count = 512650\r\n",
      "Use these ['mean_overdueamount_659A', 'max_numberofoverdueinstls_725L']\r\n",
      "####### NAN count = 263171\r\n",
      "Use these ['mean_overdueamountmax2_14A', 'mean_totaloutstanddebtvalue_39A', 'mean_dateofcredend_289D', 'mean_dateofcredstart_739D', 'max_lastupdate_1112D', 'mean_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T']\r\n",
      "####### NAN count = 262653\r\n",
      "Use these ['mean_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'mean_dateofcredend_353D', 'max_numberofoverdueinstlmax_1151L']\r\n",
      "####### NAN count = 512590\r\n",
      "Use these ['mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A']\r\n",
      "####### NAN count = 513987\r\n",
      "Use these ['max_residualamount_488A']\r\n",
      "####### NAN count = 1039597\r\n",
      "Use these ['mean_residualamount_856A']\r\n",
      "####### NAN count = 606900\r\n",
      "Use these ['max_totalamount_6A', 'mean_totalamount_6A']\r\n",
      "####### NAN count = 545855\r\n",
      "Use these ['mean_totalamount_996A']\r\n",
      "####### NAN count = 636448\r\n",
      "Use these ['mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L']\r\n",
      "####### NAN count = 297072\r\n",
      "Use these ['max_dateofrealrepmt_138D', 'mean_dateofrealrepmt_138D']\r\n",
      "####### NAN count = 512961\r\n",
      "Use these ['max_lastupdate_388D', 'mean_lastupdate_388D']\r\n",
      "####### NAN count = 512591\r\n",
      "Use these ['max_numberofoverdueinstlmaxdat_148D']\r\n",
      "####### NAN count = 802351\r\n",
      "Use these ['mean_numberofoverdueinstlmaxdat_641D']\r\n",
      "####### NAN count = 1012361\r\n",
      "Use these ['mean_overdueamountmax2date_1002D']\r\n",
      "####### NAN count = 806653\r\n",
      "Use these ['max_overdueamountmax2date_1142D']\r\n",
      "####### NAN count = 1007594\r\n",
      "####### NAN count = 553734\r\n",
      "####### NAN count = 822517\r\n",
      "####### NAN count = 745109\r\n",
      "####### NAN count = 545898\r\n",
      "####### NAN count = 636545\r\n",
      "####### NAN count = 545895\r\n",
      "####### NAN count = 636544\r\n",
      "####### NAN count = 512657\r\n",
      "####### NAN count = 561307\r\n",
      "####### NAN count = 649082\r\n",
      "Use these ['last_num_group1_6']\r\n",
      "####### NAN count = 140386\r\n",
      "Use these ['last_mainoccupationinc_384A', 'last_birth_259D']\r\n",
      "####### NAN count = 750301\r\n",
      "Use these ['max_empl_employedfrom_271D']\r\n",
      "####### NAN count = 959958\r\n",
      "Use these ['last_personindex_1023L']\r\n",
      "####### NAN count = 587206\r\n",
      "####### NAN count = 772\r\n",
      "####### NAN count = 262659\r\n",
      "####### NAN count = 512884\r\n",
      "Use these ['max_pmts_month_706T', 'max_pmts_year_507T']\r\n",
      "####### NAN count = 512598\r\n",
      "Use these ['last_pmts_month_158T', 'last_pmts_year_1139T']\r\n",
      "####### NAN count = 994041\r\n",
      "Use these ['last_pmts_month_706T', 'last_pmts_year_507T']\r\n",
      "####### NAN count = 634357\r\n",
      "Use these ['max_num_group1_13', 'max_num_group2_13', 'last_num_group2_13']\r\n",
      "####### NAN count = 141371\r\n",
      "Use these ['max_num_group1_15', 'max_num_group2_15']\r\n",
      "####### NAN count = 91554\r\n",
      "['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9', 'birthdate_574D', 'dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'mean_refreshdate_3813885D', 'pmtscount_423L', 'pmtssum_45A', 'responsedate_1012D', 'responsedate_4527233D', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'mindbdtollast24m_4525191P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'datefirstoffer_1144D', 'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'dtlastpmtallstes_4499206D', 'eir_270L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'mean_creationdate_885D', 'max_num_group1', 'last_num_group1', 'max_num_group2_14', 'last_num_group2_14', 'lastapprcredamount_781A', 'lastapprdate_640D', 'lastdelinqdate_224D', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstldate_3546855D', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'mean_actualdpd_943P', 'max_annuity_853A', 'mean_annuity_853A', 'max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A', 'max_currdebt_94A', 'mean_currdebt_94A', 'max_mainoccupationinc_437A', 'mean_mainoccupationinc_437A', 'mean_maxdpdtolerance_577P', 'max_outstandingdebt_522A', 'mean_outstandingdebt_522A', 'last_actualdpd_943P', 'last_annuity_853A', 'last_credacc_credlmt_575A', 'last_credamount_590A', 'last_downpmt_134A', 'last_currdebt_94A', 'last_mainoccupationinc_437A', 'last_maxdpdtolerance_577P', 'last_outstandingdebt_522A', 'max_approvaldate_319D', 'mean_approvaldate_319D', 'max_dateactivated_425D', 'mean_dateactivated_425D', 'max_dtlastpmt_581D', 'mean_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'mean_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'mean_firstnonzeroinstldate_307D', 'last_approvaldate_319D', 'last_creationdate_885D', 'last_dateactivated_425D', 'last_dtlastpmtallstes_3545839D', 'last_employedfrom_700D', 'last_firstnonzeroinstldate_307D', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'last_pmtnum_8L', 'max_pmtamount_36A', 'last_pmtamount_36A', 'max_processingdate_168D', 'last_processingdate_168D', 'max_num_group1_5', 'mean_credlmt_230A', 'mean_credlmt_935A', 'mean_pmts_dpd_1073P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'max_pmts_dpd_303P', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P', 'mean_instlamount_768A', 'mean_monthlyinstlamount_332A', 'max_monthlyinstlamount_674A', 'mean_monthlyinstlamount_674A', 'mean_outstandingamount_354A', 'mean_outstandingamount_362A', 'mean_overdueamount_31A', 'mean_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'mean_overdueamountmax2_14A', 'mean_totaloutstanddebtvalue_39A', 'mean_dateofcredend_289D', 'mean_dateofcredstart_739D', 'max_lastupdate_1112D', 'mean_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'mean_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'mean_dateofcredend_353D', 'max_numberofoverdueinstlmax_1151L', 'mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A', 'max_residualamount_488A', 'mean_residualamount_856A', 'max_totalamount_6A', 'mean_totalamount_6A', 'mean_totalamount_996A', 'mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_dateofrealrepmt_138D', 'mean_dateofrealrepmt_138D', 'max_lastupdate_388D', 'mean_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'mean_numberofoverdueinstlmaxdat_641D', 'mean_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'last_refreshdate_3813885D', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'last_num_group1_6', 'last_mainoccupationinc_384A', 'last_birth_259D', 'max_empl_employedfrom_271D', 'last_personindex_1023L', 'last_persontype_1072L', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'last_pmts_month_158T', 'last_pmts_year_1139T', 'last_pmts_month_706T', 'last_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'last_num_group2_13', 'max_num_group1_15', 'max_num_group2_15']\r\n",
      "276\r\n",
      "389\r\n",
      "test data shape:\t (10, 860)\r\n",
      "train data shape:\t (50000, 389)\r\n",
      "test data shape:\t (10, 388)\r\n",
      "Memory usage of dataframe is 0.04 MB\r\n",
      "Memory usage after optimization is: 0.02 MB\r\n",
      "Decreased by 40.3%\r\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\r\n",
      "0:\ttest: 0.5792207\tbest: 0.5792207 (0)\ttotal: 305ms\tremaining: 3m 2s\r\n",
      "300:\ttest: 0.7541151\tbest: 0.7546397 (280)\ttotal: 27.6s\tremaining: 27.5s\r\n",
      "599:\ttest: 0.7560412\tbest: 0.7574959 (380)\ttotal: 54.6s\tremaining: 0us\r\n",
      "bestTest = 0.7574959397\r\n",
      "bestIteration = 380\r\n",
      "Shrink model to first 381 iterations.\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "Early stopping, best iteration is:\r\n",
      "[86]\tvalid_0's auc: 0.765434\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "[200]\tvalid_0's auc: 0.757262\r\n",
      "Early stopping, best iteration is:\r\n",
      "[143]\tvalid_0's auc: 0.7619\r\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\r\n",
      "0:\ttest: 0.5796317\tbest: 0.5796317 (0)\ttotal: 144ms\tremaining: 1m 26s\r\n",
      "300:\ttest: 0.7438381\tbest: 0.7441371 (295)\ttotal: 27.3s\tremaining: 27.2s\r\n",
      "599:\ttest: 0.7494330\tbest: 0.7494330 (599)\ttotal: 54.1s\tremaining: 0us\r\n",
      "bestTest = 0.7494330108\r\n",
      "bestIteration = 599\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "Early stopping, best iteration is:\r\n",
      "[111]\tvalid_0's auc: 0.754794\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "Early stopping, best iteration is:\r\n",
      "[80]\tvalid_0's auc: 0.754695\r\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\r\n",
      "0:\ttest: 0.5679812\tbest: 0.5679812 (0)\ttotal: 136ms\tremaining: 1m 21s\r\n",
      "300:\ttest: 0.7204567\tbest: 0.7204567 (300)\ttotal: 27.3s\tremaining: 27.1s\r\n",
      "599:\ttest: 0.7211076\tbest: 0.7217967 (545)\ttotal: 54.2s\tremaining: 0us\r\n",
      "bestTest = 0.7217966914\r\n",
      "bestIteration = 545\r\n",
      "Shrink model to first 546 iterations.\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "Early stopping, best iteration is:\r\n",
      "[95]\tvalid_0's auc: 0.729991\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "Early stopping, best iteration is:\r\n",
      "[92]\tvalid_0's auc: 0.730187\r\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\r\n",
      "0:\ttest: 0.6325521\tbest: 0.6325521 (0)\ttotal: 134ms\tremaining: 1m 20s\r\n",
      "300:\ttest: 0.7567488\tbest: 0.7567488 (300)\ttotal: 27s\tremaining: 26.9s\r\n",
      "599:\ttest: 0.7625934\tbest: 0.7625934 (599)\ttotal: 54.3s\tremaining: 0us\r\n",
      "bestTest = 0.7625933886\r\n",
      "bestIteration = 599\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "Early stopping, best iteration is:\r\n",
      "[73]\tvalid_0's auc: 0.764708\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "Early stopping, best iteration is:\r\n",
      "[85]\tvalid_0's auc: 0.766373\r\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\r\n",
      "0:\ttest: 0.5882546\tbest: 0.5882546 (0)\ttotal: 144ms\tremaining: 1m 26s\r\n",
      "300:\ttest: 0.7128673\tbest: 0.7128673 (300)\ttotal: 27.1s\tremaining: 27s\r\n",
      "599:\ttest: 0.7200133\tbest: 0.7201119 (595)\ttotal: 54s\tremaining: 0us\r\n",
      "bestTest = 0.7201119065\r\n",
      "bestIteration = 595\r\n",
      "Shrink model to first 596 iterations.\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "[200]\tvalid_0's auc: 0.723181\r\n",
      "Early stopping, best iteration is:\r\n",
      "[192]\tvalid_0's auc: 0.723802\r\n",
      "Training until validation scores don't improve for 60 rounds\r\n",
      "[200]\tvalid_0's auc: 0.720869\r\n",
      "Early stopping, best iteration is:\r\n",
      "[157]\tvalid_0's auc: 0.722364\r\n",
      "CatBoost\r\n",
      "CV AUC scores:  [0.7574952064152896, 0.7494331991423062, 0.7217970257898471, 0.7625935209294905, 0.720112098726241]\r\n",
      "Maximum CV AUC score:  0.7625935209294905\r\n",
      "LightGBM\r\n",
      "CV AUC scores:  [0.7654337093743829, 0.7547939526010682, 0.7299913448565538, 0.7647079659417527, 0.7238016348557597]\r\n",
      "Maximum CV AUC score:  0.7654337093743829\r\n",
      "LightGBM_goss\r\n",
      "CV AUC scores:  [0.7619000647629788, 0.7546948380468184, 0.7301873257630201, 0.7663727534021745, 0.722364191897147]\r\n",
      "Maximum CV AUC score:  0.7663727534021745\r\n",
      "Ensemble of LGBM and LGBM_goss\r\n",
      "CV AUC scores:  [0.7092492267368219, 0.6865876841262835, 0.6753661954639476, 0.7253289962875537, 0.6843145113289155]\r\n",
      "Maximum CV AUC score:  0.7253289962875537\r\n"
     ]
    }
   ],
   "source": [
    "!python baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33e0d5f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:48:55.616543Z",
     "iopub.status.busy": "2024-05-26T13:48:55.615612Z",
     "iopub.status.idle": "2024-05-26T13:48:59.196112Z",
     "shell.execute_reply": "2024-05-26T13:48:59.195291Z"
    },
    "papermill": {
     "duration": 3.604355,
     "end_time": "2024-05-26T13:48:59.198465",
     "exception": false,
     "start_time": "2024-05-26T13:48:55.594110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbfbe6fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:48:59.238576Z",
     "iopub.status.busy": "2024-05-26T13:48:59.238274Z",
     "iopub.status.idle": "2024-05-26T13:49:00.304190Z",
     "shell.execute_reply": "2024-05-26T13:49:00.303242Z"
    },
    "papermill": {
     "duration": 1.088224,
     "end_time": "2024-05-26T13:49:00.306472",
     "exception": false,
     "start_time": "2024-05-26T13:48:59.218248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train,y,df_test=joblib.load('/kaggle/working/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f77a7f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:49:00.389134Z",
     "iopub.status.busy": "2024-05-26T13:49:00.388771Z",
     "iopub.status.idle": "2024-05-26T13:50:41.589826Z",
     "shell.execute_reply": "2024-05-26T13:50:41.588960Z"
    },
    "papermill": {
     "duration": 101.26627,
     "end_time": "2024-05-26T13:50:41.592448",
     "exception": false,
     "start_time": "2024-05-26T13:49:00.326178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10, number of negative: 1526659\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.100954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46717\n",
      "[LightGBM] [Info] Number of data points in the train set: 1526669, number of used features: 308\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000007 -> initscore=-11.936007\n",
      "[LightGBM] [Info] Start training from score -11.936007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "fitted_models_lgb=[]\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(df_train,y)\n",
    "fitted_models_lgb.append(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da973b34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:50:41.637628Z",
     "iopub.status.busy": "2024-05-26T13:50:41.637063Z",
     "iopub.status.idle": "2024-05-26T13:50:41.644253Z",
     "shell.execute_reply": "2024-05-26T13:50:41.643371Z"
    },
    "papermill": {
     "duration": 0.031513,
     "end_time": "2024-05-26T13:50:41.646162",
     "exception": false,
     "start_time": "2024-05-26T13:50:41.614649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "model = VotingModel(fitted_models_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6cb772e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:50:41.690673Z",
     "iopub.status.busy": "2024-05-26T13:50:41.690405Z",
     "iopub.status.idle": "2024-05-26T13:50:41.694189Z",
     "shell.execute_reply": "2024-05-26T13:50:41.693459Z"
    },
    "papermill": {
     "duration": 0.028708,
     "end_time": "2024-05-26T13:50:41.696580",
     "exception": false,
     "start_time": "2024-05-26T13:50:41.667872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set threshold and correction values\n",
    "threshold = 0.998\n",
    "correction = 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6364ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T13:50:41.740468Z",
     "iopub.status.busy": "2024-05-26T13:50:41.740194Z",
     "iopub.status.idle": "2024-05-26T13:50:41.783008Z",
     "shell.execute_reply": "2024-05-26T13:50:41.782145Z"
    },
    "papermill": {
     "duration": 0.066855,
     "end_time": "2024-05-26T13:50:41.785017",
     "exception": false,
     "start_time": "2024-05-26T13:50:41.718162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57543</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57549</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57551</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57552</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57569</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57630</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57631</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57632</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57633</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57634</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score\n",
       "case_id       \n",
       "57543      0.0\n",
       "57549      0.0\n",
       "57551      0.0\n",
       "57552      0.0\n",
       "57569      0.0\n",
       "57630      0.0\n",
       "57631      0.0\n",
       "57632      0.0\n",
       "57633      0.0\n",
       "57634      0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.drop(columns=[\"WEEK_NUM\",'target'])\n",
    "df_test = df_test.set_index(\"case_id\")\n",
    "\n",
    "y_pred = pd.Series(model.predict_proba(df_test)[:,1], index=df_test.index)\n",
    "df_subm = pd.read_csv(\"/kaggle/working/sub.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm.loc[y_pred < threshold, 'score'] = (df_subm.loc[y_pred < threshold, 'score'] * threshold - correction).clip(0)\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "df_subm"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1930.263075,
   "end_time": "2024-05-26T13:50:42.631262",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-26T13:18:32.368187",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
